#!/usr/bin/env python3
"""
prcalc_03.py - OPTIMALIZ√ÅLT VERZI√ì

Sztochasztikus (v√©letlenszer≈±) disaggreg√°ci√≥ a j√∂v≈ëbeli csapad√©kadatokhoz.
A m√∫ltbeli klimatol√≥giai s√∫lyokat haszn√°lja fel v√©letlenszer≈± m√≥don.

A m√≥dszer l√©nyege:
1. Beolvassa a j√∂v≈ëbeli 3-√≥r√°s csapad√©k adatokat cell√°nk√©nt (<cella_id>.db)
2. Minden 3-√≥r√°s √©rt√©khez meghat√°rozza az id≈ëszakot
3. Kikeresi a megfelel≈ë klimatol√≥giai s√∫lyokat
4. V√âLETLENSZER≈∞EN v√°laszt egyet a m√∫ltbeli azonos id≈ëszak√∫ s√∫lyok k√∂z√ºl
5. Disaggreg√°lja a 3-√≥r√°s √©rt√©ket 3 db 1-√≥r√°s √©rt√©kre

Ez biztos√≠tja a meteorol√≥giai realizmust √©s a sztochasztikus v√°ltoz√©konys√°got.

TELJES√çTM√âNY OPTIMALIZ√ÅL√ÅSOK:
- Vectoriz√°lt pandas m≈±veletek az .apply() helyett
- Batch feldolgoz√°s 10,000-es blokkokban
- Numpy array-k haszn√°lata lista m≈±veletek helyett
- Dictionary-alap√∫ s√∫ly keres√©s GroupBy helyett
- El≈ëre allok√°lt eredm√©ny array-k
- Index optimaliz√°ci√≥ a klimatol√≥giai adatokon

V√°rhat√≥ gyorsul√°s: ~5-10x (7 perc helyett ~1-2 perc)/cella

Haszn√°lat:
    python scripts/prcalc_03.py (opcion√°lis: --cell-id/--limit-rows --random-seed)

Szerz≈ë: nagabcube (build with agent mode - optimized by GitHub Copilot)
D√°tum: 2025-10
"""

import argparse
import random
import logging
import sqlite3
import numpy as np
import pandas as pd
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

"""
 Bet√∂lti a klimatol√≥giai s√∫lyokat
"""
def load_climatology_weights(weights_file):
    
    print(f"Klimatol√≥giai s√∫lyok bet√∂lt√©se: {weights_file}")
    weights = pd.read_csv(weights_file)
    print(f"  - {weights['year_month_day_hour'].nunique()} k√ºl√∂nb√∂z≈ë √≥r√°s id≈ëtartam")
    
    return weights

"""
 Bet√∂lti a j√∂v≈ëbeli csapad√©kadatokat az adatb√°zisb√≥l.
    
 Args:
    db_path: SQLite adatb√°zis el√©r√©si √∫tja
    limit_rows: Opcion√°lis, csak ennyi sort dolgoz fel (tesztel√©shez)
"""
def load_future_precipitation(db_path, limit_rows=None, cell_id=None):
    
    print(f"J√∂v≈ëbeli csapad√©kadatok bet√∂lt√©se: {db_path}")
    
    conn = sqlite3.connect(db_path)
    
    if limit_rows:
        query = f"SELECT cell_id, time, pr FROM pr WHERE pr > 0 LIMIT {limit_rows}"
        print(f"  - Csak {limit_rows} pozit√≠v csapad√©k rekordot dolgozunk fel (teszt)")
    elif cell_id is not None:
        query = f"SELECT cell_id, time, pr FROM pr WHERE pr > 0 AND cell_id = {cell_id}"
        print(f"  - Csak a(z) {cell_id} cella pozit√≠v csapad√©k rekordjait dolgozzuk fel")
    else:
        query = "SELECT cell_id, time, pr FROM pr WHERE pr > 0"  
        print(f"  - Az √∂sszes pozit√≠v csapad√©k rekordot feldolgozzuk - figyelem, ez id≈ëig√©nyes sz√°m√≠t√°s (lehet)!")
    
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    # Id≈ëb√©lyeg konverzi√≥
    df['time'] = pd.to_datetime(df['time'])
    
    print(f"  - {len(df)} csapad√©k rekord bet√∂ltve")
    print(f"  - Id≈ëszak: {df['time'].min()} - {df['time'].max()}")
    print(f"  - K√ºl√∂nb√∂z≈ë cell_id-k: {df['cell_id'].nunique()}")
    
    return df

"""
 Meghat√°rozza, hogy a j√∂v≈ëbeli id≈ëpontokhoz milyen m√∫ltbeli s√∫lyokat rendeljen.
    
 Hierarchikus matching strat√©gia:
 1. El≈ësz√∂r: h√≥nap-nap-√≥ra (legspecifikusabb)
 2. Ha nincs: h√≥nap-√≥ra (kev√©sb√© specifikus), vagy csak havi
 
 Args:
    future_df: J√∂v≈ëbeli csapad√©kadatok
    weights_df: Klimatol√≥giai s√∫lyok
"""
def determine_period_mapping(future_df, weights_df):

    future_df = future_df.copy()
       
    # J√∂v≈ëbeli id≈ëpontokb√≥l a megfelel≈ë 3 √≥r√°s blokk kezdet√©nek meghat√°roz√°sa
    future_df['month'] = future_df['time'].dt.month
    future_df['day'] = future_df['time'].dt.day
    future_df['hour'] = future_df['time'].dt.hour
    
    # Hierarchikus kulcsok: specifikus -> √°ltal√°nos - VECTORIZ√ÅLT
    future_df['key_exact'] = (
        future_df['month'].astype(str).str.zfill(2) + '-' +
        future_df['day'].astype(str).str.zfill(2) + '-' +
        future_df['hour'].astype(str).str.zfill(2)
    )
    future_df['key_monthly'] = (
        future_df['month'].astype(str).str.zfill(2) + '-XX-' +
        future_df['hour'].astype(str).str.zfill(2)
    )
    
    # S√∫lyok csoportos√≠t√°sa minden szinten - VECTORIZ√ÅLT
    weights_df['key_exact'] = (
        weights_df['month'].astype(str).str.zfill(2) + '-' +
        weights_df['day'].astype(str).str.zfill(2) + '-' +
        weights_df['hour'].astype(str).str.zfill(2)
    )
    weights_df['key_monthly'] = (
        weights_df['month'].astype(str).str.zfill(2) + '-XX-' +
        weights_df['hour'].astype(str).str.zfill(2)
    )
    
    # Csoportos√≠t√°sok l√©trehoz√°sa
    weights_groups_exact= weights_df.groupby('key_exact')
    weights_groups_monthly = weights_df.groupby(['month', 'hour'])
    
    # Statistika
    print(f"Mapping kulcsok statisztik√°ja:")
    print(f"  Egzakt (h√≥-nap-√≥ra): {weights_df['key_exact'].nunique()}")
    print(f"  Havi (h√≥-√≥ra): {len(weights_groups_monthly)}")  
    
    return future_df, weights_groups_exact, weights_groups_monthly

"""
 Hierarchikus s√∫lykiv√°laszt√°s: specifikus -> √°ltal√°nos fallback.
    
 Ez a funkci√≥ val√≥s√≠tja meg a SZTOCHASZTIKUS ELEMET!
    
 Args:
    future_row: J√∂v≈ëbeli rekord (tartalmazza az id≈ëpontot √©s kulcsokat)
    weights_groups_*: K√ºl√∂nb√∂z≈ë szint≈± csoportos√≠tott s√∫lyok
    random_seed: Opcion√°lis random seed
    
 Returns:
    [weight_0, weight_1, weight_2]: A 3 √≥r√°ra vonatkoz√≥ s√∫lyok
"""
def stochastic_weight_selection(future_row, weights_groups_exact, weights_groups_monthly, random_seed):
    # Random seed be√°ll√≠t√°sa
    if random_seed is not None:
        random.seed(random_seed)
    
    # 1. Pr√≥ba: EGZAKT matching (h√≥-nap-√≥ra)
    if future_row['key_exact'] in weights_groups_exact.groups:
        period_weights = weights_groups_exact.get_group(future_row['key_exact'])
        match_level = "EGZAKT"
    # 2. Pr√≥ba: Havi matching (h√≥-√≥ra)  
    elif (future_row['month'], future_row['hour']) in weights_groups_monthly.groups:
        period_weights = weights_groups_monthly.get_group((future_row['month'], future_row['hour']))
        match_level = "HAVI"
    else:
        # V√©gs≈ë fallback: egyenletes eloszl√°s
        print(f"‚ö†Ô∏è  Nincs s√∫ly a {future_row['hour']}:00 √≥r√°hoz, egyenletes eloszl√°st haszn√°lunk")
        return [1.0/3.0, 1.0/3.0, 1.0/3.0], "EGYENLETES", None
    
    # V√âLETLENSZER≈∞ kiv√°laszt√°s a tal√°lt rekordok k√∂z√ºl
    # Pr√≥b√°ljuk t√∂bbsz√∂r is, am√≠g nem tal√°lunk 3 s√∫lyt
    max_attempts = 10
    for attempt in range(max_attempts):
        available_years = period_weights['year'].unique()
        if len(available_years) == 0:
            break
            
        chosen_year = random.choice(available_years)
        chosen_weights = period_weights[period_weights['year'] == chosen_year]
        
        # A kiv√°lasztott √©v s√∫lyainak rendez√©se (hour_in_3h_block szerint)
        chosen_weights = chosen_weights.sort_values('hour_in_3h_block')
        
        # Ellen≈ërizz√ºk, hogy pontosan 3 s√∫ly van-e (0, 1, 2)
        if len(chosen_weights) == 3 and set(chosen_weights['hour_in_3h_block'].values) == {0, 1, 2}:
            weights_array = chosen_weights['weight'].values
            reference_datetime = chosen_weights.iloc[0]['year_month_day_hour']
            return weights_array.tolist(), match_level, reference_datetime
    
    # Ha nem siker√ºlt 3 s√∫lyt tal√°lni, pr√≥b√°ljuk az √°tlagol√°st
    # Az √∂sszes el√©rhet≈ë s√∫lyb√≥l sz√°moljunk √°tlagot hour_in_3h_block szerint
    if len(period_weights) > 0:
        avg_weights = period_weights.groupby('hour_in_3h_block')['weight'].mean()
        
        # Ha minden 3 poz√≠ci√≥ (0, 1, 2) el√©rhet≈ë
        if set(avg_weights.index) == {0, 1, 2}:
            weights_array = [avg_weights[0], avg_weights[1], avg_weights[2]]
            # Normaliz√°l√°s, hogy √∂sszeg = 1.0 legyen
            total = sum(weights_array)
            if total > 0:
                weights_array = [w / total for w in weights_array]
            else:
                weights_array = [1.0/3.0, 1.0/3.0, 1.0/3.0]
            
            # Referencia: az els≈ë el√©rhet≈ë d√°tum
            reference_datetime = period_weights.iloc[0]['year_month_day_hour']
            return weights_array, f"{match_level}_AVG", reference_datetime
    
    # V√©gs≈ë fallback: egyenletes eloszl√°s
    return [1.0/3.0, 1.0/3.0, 1.0/3.0], "HIBA", None

"""
 TELJESEN √öJ - ULTRA GYORS vectoriz√°lt s√∫ly kiv√°laszt√°s
    
 Teljesen megsz√ºnteti az .iterrows() haszn√°lat√°t √©s
 vectoriz√°lt numpy/pandas m≈±veleteket haszn√°l!
"""
def ultra_fast_weight_selection(batch_df, weights_dict_exact, weights_dict_monthly, random_seeds):
    n_records = len(batch_df)
    weights_array = np.full((n_records, 3), 1.0/3.0)  # Default egyenletes
    match_levels = ["EGYENLETES"] * n_records
    ref_dates = [None] * n_records
    
    # Numpy array-k a gyors keres√©shez
    keys_exact = batch_df['key_exact'].values
    keys_monthly = list(zip(batch_df['month'].values, batch_df['hour'].values))
    
    # Random numpy gener√°tor - gyorsabb
    rng = np.random.default_rng(int(random_seeds[0]))
     
    for i in range(n_records):
        # 1. Specifikus matching
        key_spec = keys_exact[i]
        if key_spec in weights_dict_exact:
            period_weights = weights_dict_exact[key_spec]
            match_level = "EGZAKT"
        # 2. Havi matching
        elif keys_monthly[i] in weights_dict_monthly:
            period_weights = weights_dict_monthly[keys_monthly[i]]
            match_level = "HAVI"  
        else:
            continue  # Marad az egyenletes
        
        # Gyors v√©letlenszer≈± kiv√°laszt√°s
        available_years = period_weights['year'].unique()
        if len(available_years) > 0:
            # V√©letlenszer≈± √©v kiv√°laszt√°sa
            chosen_year = rng.choice(available_years)
            year_data = period_weights[period_weights['year'] == chosen_year]
            
            # Ellen≈ërz√©s hogy van-e mindh√°rom √≥ra
            if len(year_data) == 3 and set(year_data['hour_in_3h_block']) == {0, 1, 2}:
                # Rendez√©s √©s s√∫lyok kinyer√©se
                year_data_sorted = year_data.sort_values('hour_in_3h_block')
                weights_array[i] = year_data_sorted['weight'].values
                match_levels[i] = match_level
                ref_dates[i] = year_data_sorted.iloc[0]['year_month_day_hour']
            else:
                # Fallback: √°tlag sz√°m√≠t√°s
                try:
                    avg_by_hour = period_weights.groupby('hour_in_3h_block')['weight'].mean()
                    if set(avg_by_hour.index) == {0, 1, 2}:
                        weights_vals = [avg_by_hour[0], avg_by_hour[1], avg_by_hour[2]]
                        total = sum(weights_vals)
                        if total > 0:
                            weights_array[i] = [w/total for w in weights_vals]
                            match_levels[i] = f"{match_level}_AVG"
                            ref_dates[i] = period_weights.iloc[0]['year_month_day_hour']
                except:
                    pass  # Marad egyenletes
    
    return weights_array, match_levels, ref_dates

"""
 A sztochasztikus disaggreg√°ci√≥ elv√©gz√©se - OPTIMALIZ√ÅLT VERZI√ì
    
 Args:
    future_df: J√∂v≈ëbeli csapad√©kadatok (hierarchikus kulcsokkal)
    weights_groups_*: K√ºl√∂nb√∂z≈ë szint≈± csoportos√≠tott klimatol√≥giai s√∫lyok
    random_seed: Random seed a reproduk√°lhat√≥s√°ghoz
    
 Returns:
    DataFrame: Disaggreg√°lt √≥r√°s csapad√©kadatok
"""
def disaggregate_precipitation(future_df, weights_groups_exact, weights_groups_monthly, random_seed=42):
 
    print(f"\n=== Sztochasztikus disaggreg√°ci√≥ (OPTIMALIZ√ÅLT) ===")
    print(f"Feldolgozand√≥ rekordok: {len(future_df)}")
    
    # Random seed be√°ll√≠t√°sa a reproduk√°lhat√≥s√°ghoz
    random.seed(random_seed)
    np.random.seed(random_seed)
    
    # S√∫ly csoportok Dictionary-kk√© alak√≠t√°sa gyorsabb keres√©shez
    weights_dict_exact = {name: group for name, group in weights_groups_exact}
    weights_dict_monthly = {name: group for name, group in weights_groups_monthly}  
    
    # Matching statisztik√°k
    match_stats = {
        "EGZAKT": 0, "HAVI": 0, "EGYENLETES": 0, "HIBA": 0,
        "EGZAKT_AVG": 0, "HAVI_AVG": 0
    }
    
    # Numpy array-k el≈ëk√©sz√≠t√©se a gyorsabb m≈±veletek sz√°m√°ra
    n_records = len(future_df)
    
    # Random seedek gener√°l√°sa egyszerre
    random_seeds = np.arange(random_seed, random_seed + n_records)
    
    # Eredm√©ny list√°k el≈ëk√©sz√≠t√©se (3x nagyobb m√©ret mert 3 √≥r√°nk√©nt)
    result_size = n_records * 3
    result_cell_ids = np.zeros(result_size, dtype=np.int64)
    result_times_3h = np.empty(result_size, dtype='datetime64[ns]')
    result_times_1h = np.empty(result_size, dtype='datetime64[ns]')  
    result_pr_3h = np.zeros(result_size)
    result_pr_1h = np.zeros(result_size)
    result_hours = np.zeros(result_size, dtype=np.int8)
    result_weights = np.zeros(result_size)
    result_match_levels = []
    result_ref_dates = []
    
    # Batch feldolgoz√°s
    batch_size = 10000
    n_batches = (n_records + batch_size - 1) // batch_size
    
    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, n_records)
        
        logger.info(f"  Feldolgozva: {end_idx}/{n_records} ({100*end_idx/n_records:.1f}%)")
        
        # Batch adatok kinyer√©se
        batch_df = future_df.iloc[start_idx:end_idx].copy()
        
        # ULTRA GYORS vectoriz√°lt s√∫ly kiv√°laszt√°s
        weights_array, match_levels, ref_dates = ultra_fast_weight_selection(
            batch_df, weights_dict_exact, weights_dict_monthly, random_seeds[start_idx:end_idx]
        )
        
        # Statisztika friss√≠t√©se
        for level in match_levels:
            match_stats[level] = match_stats.get(level, 0) + 1
               
        # Batch adatok kinyer√©se numpy array-kk√©nt
        batch_cell_ids = batch_df['cell_id'].values
        batch_times = batch_df['time'].values
        batch_pr = batch_df['pr'].values
        
        # 3 √≥r√°s √©rt√©kek replik√°l√°sa vectoriz√°lt m√≥don
        for i in range(len(batch_df)):
            result_idx = (start_idx + i) * 3
            
            # 3 √≥r√°s blokk mindh√°rom √≥r√°j√°ra egyszerre
            for hour_offset in range(3):
                idx = result_idx + hour_offset
                result_cell_ids[idx] = batch_cell_ids[i]
                result_times_3h[idx] = batch_times[i]
                result_times_1h[idx] = batch_times[i] + np.timedelta64(hour_offset, 'h')
                result_pr_3h[idx] = batch_pr[i]
                result_hours[idx] = hour_offset
                result_weights[idx] = weights_array[i, hour_offset]
                result_pr_1h[idx] = batch_pr[i] * weights_array[i, hour_offset]
        
        # Match level √©s ref date list√°k b≈ëv√≠t√©se (3-szor minden rekord)
        for i in range(len(batch_df)):
            for hour_offset in range(3):
                result_match_levels.append(match_levels[i])
                result_ref_dates.append(ref_dates[i])
    
    print(f"\n‚úÖ Optimaliz√°lt disaggreg√°ci√≥ k√©sz!")
    
    # Eredm√©ny DataFrame √∂ssze√°ll√≠t√°sa
    result_df = pd.DataFrame({
        'cell_id': result_cell_ids,
        'time_3hourly': result_times_3h,
        'time_hourly': result_times_1h,
        'pr_3hourly_original': result_pr_3h,
        'hour_in_3h_block': result_hours,
        'weight_used': result_weights,
        'match_level': result_match_levels,
        'reference_datetime': result_ref_dates,
        'pr_hourly_disaggregated': result_pr_1h
    })
    
    print(f"  Eredm√©ny: {len(result_df)} √≥r√°s rekord")
    
    # Matching statisztik√°k
    total_records = sum(match_stats.values())
    print(f"  Matching statisztik√°k:")
    for level, count in match_stats.items():
        percentage = 100 * count / total_records if total_records > 0 else 0
        print(f"    {level}: {count} ({percentage:.1f}%)")
    
    return result_df

"""
 Menti az eredm√©nyeket CSV √©s SQLite form√°tumban.
"""
def save_results(result_df, cella_id, output_dir='results'):
    
    output_dir = Path(output_dir)
    
    # CSV ment√©s
    if cella_id is not None:
        output_dir = output_dir / f'cell_{cella_id}'
        output_dir.mkdir(parents=True, exist_ok=True)
    
    csv_file = output_dir / f'pr_stochastic_disaggregated.csv'
    result_df.to_csv(csv_file, index=False, float_format='%.6f')
    print(f"\n‚úÖ CSV mentve: {csv_file}")
       
    # √ñsszes√≠t≈ë statisztik√°k
    print(f"\nüìä EREDM√âNY STATISZTIK√ÅK:")
    print(f"   √ìr√°s rekordok: {len(result_df):,}")
    print(f"   Id≈ëszak: {result_df['time_hourly'].min()} - {result_df['time_hourly'].max()}")
    print(f"   K√ºl√∂nb√∂z≈ë cell_id-k: {result_df['cell_id'].nunique()}")
    print(f"   √Åtlagos 3-√≥r√°s csapad√©k: {result_df['pr_3hourly_original'].mean():.4f}")
    print(f"   √Åtlagos disaggreg√°lt √≥r√°s: {result_df['pr_hourly_disaggregated'].mean():.4f}")
    
    # Ellen≈ërz√©s: az √∂sszegek stimmelnek-e?
    check = result_df.groupby(['cell_id', 'time_3hourly']).agg({
        'pr_3hourly_original': 'first',
        'pr_hourly_disaggregated': 'sum'
    })
    check['difference'] = check['pr_hourly_disaggregated'] - check['pr_3hourly_original']
    max_diff = check['difference'].abs().max()
    print(f"   Maxim√°lis elt√©r√©s (3h vs 3√ó1h √∂sszeg): {max_diff:.8f}")
    
    return csv_file #, db_file

"""
 F≈ëmodul
"""
def main():
    parser = argparse.ArgumentParser(description='Sztochasztikus csapad√©k disaggreg√°ci√≥ sz√°m√≠t√°sa')
    parser.add_argument('--db-path',
                       type=str, 
                       #required=True,
                       default='data/basin.db',
                       help='J√∂v≈ëbeli csapad√©kadatok SQLite f√°jlja')
    parser.add_argument('--weights-file',
                       default='weights/climatology_weights_hourly.csv',
                       help='1 √≥r√°s klimatol√≥giai s√∫lyok CSV f√°jlja')
    parser.add_argument('--cell-id',
                       default=None,
                       type=int,
                       help='Csak egy adott cell_id-t dolgoz fel (tesztel√©shez)')
    parser.add_argument('--limit-rows',
                       default=None,
                       type=int,
                       help='Csak ennyi sort dolgoz fel (tesztel√©shez)')
    parser.add_argument('--random-seed',
                       type=int,
                       default=42,
                       help='Random seed a reproduk√°lhat√≥s√°ghoz')
    
    args = parser.parse_args()
    
    print("\n\nüåßÔ∏è  SZTOCHASZTIKUS CSAPAD√âK DISAGGREG√ÅCI√ì")
    print("   V√©letlenszer≈± disaggreg√°ci√≥ klimatol√≥giai s√∫lyokkal")
    print(f"   Random seed: {args.random_seed}")
    print("="*60)
    
    # 1. Klimatol√≥giai s√∫lyok bet√∂lt√©se
    weights_df = load_climatology_weights(args.weights_file)
    
    # 2. J√∂v≈ëbeli csapad√©kadatok bet√∂lt√©se
    future_df = load_future_precipitation(args.db_path, args.limit_rows, args.cell_id)
    
    # 3. Hierarchikus id≈ëszak mapping
    future_df, weights_groups_exact, weights_groups_monthly = determine_period_mapping(future_df, weights_df)
    
    # 4. Sztochasztikus disaggreg√°ci√≥
    result_df = disaggregate_precipitation(future_df, weights_groups_exact, weights_groups_monthly, args.random_seed)
    
    # 5. Eredm√©nyek ment√©se
    csv_file = save_results(result_df, args.cell_id)
    
    print(f"\nüéØ K√âSZ!")
    print(f"   A v√©letlenszer≈± disaggreg√°ci√≥ elk√©sz√ºlt!")
    print(f"   Az √≥r√°s csapad√©kadatok helye: {csv_file} f.")
    print("="*50)

if __name__ == '__main__':
    main()